{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ualNlhI7ZaJk"
      },
      "outputs": [],
      "source": [
        "# # Install necessary libraries for Unsloth, Hugging Face, etc.\n",
        "# # Uses specific versions optimized for Colab in the 'else' block.\n",
        "# %%capture\n",
        "# import os\n",
        "# if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "#     !pip install unsloth\n",
        "# else:\n",
        "#     # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "#     !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "#     !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Necessary libraries installed.\n"
          ]
        }
      ],
      "source": [
        "print(\"Necessary libraries installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes==0.43.1\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting peft==0.11.1\n",
            "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl==0.9.4\n",
            "  Downloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting transformers==4.41.2\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting xformers==0.0.26.post1\n",
            "  Downloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: torch in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from bitsandbytes==0.43.1) (2.5.1)\n",
            "Requirement already satisfied: numpy in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from bitsandbytes==0.43.1) (1.22.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from peft==0.11.1) (25.0)\n",
            "Requirement already satisfied: psutil in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from peft==0.11.1) (5.9.1)\n",
            "Requirement already satisfied: pyyaml in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from peft==0.11.1) (6.0)\n",
            "Requirement already satisfied: tqdm in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from peft==0.11.1) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from peft==0.11.1) (1.11.0)\n",
            "Requirement already satisfied: safetensors in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from peft==0.11.1) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from peft==0.11.1) (0.36.0)\n",
            "Requirement already satisfied: datasets in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from trl==0.9.4) (4.3.0)\n",
            "Collecting tyro>=0.5.11 (from trl==0.9.4)\n",
            "  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from transformers==4.41.2) (3.20.0)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.41.2)\n",
            "  Downloading regex-2025.10.23-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: requests in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from transformers==4.41.2) (2.32.5)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.2)\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting torch (from bitsandbytes==0.43.1)\n",
            "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (1.14.0)\n",
            "Requirement already satisfied: networkx in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (3.4)\n",
            "Requirement already satisfied: jinja2 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (2025.9.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.0 (from torch->bitsandbytes==0.43.1)\n",
            "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes==0.43.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (1.1.8)\n",
            "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.9.4)\n",
            "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.9.4)\n",
            "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.9.4)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.9.4)\n",
            "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.4)\n",
            "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.4) (2.19.1)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.4)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from datasets->trl==0.9.4) (21.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from datasets->trl==0.9.4) (0.4.0)\n",
            "Requirement already satisfied: pandas in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from datasets->trl==0.9.4) (1.4.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from datasets->trl==0.9.4) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from datasets->trl==0.9.4) (0.0.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from datasets->trl==0.9.4) (0.70.12.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->trl==0.9.4) (3.8.1)\n",
            "Requirement already satisfied: anyio in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from httpx<1.0.0->datasets->trl==0.9.4) (4.11.0)\n",
            "Requirement already satisfied: certifi in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from httpx<1.0.0->datasets->trl==0.9.4) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from httpx<1.0.0->datasets->trl==0.9.4) (1.0.9)\n",
            "Requirement already satisfied: idna in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from httpx<1.0.0->datasets->trl==0.9.4) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets->trl==0.9.4) (0.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->trl==0.9.4) (25.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->trl==0.9.4) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->trl==0.9.4) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->trl==0.9.4) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->trl==0.9.4) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->trl==0.9.4) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->trl==0.9.4) (1.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from requests->transformers==4.41.2) (2.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets->trl==0.9.4) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets->trl==0.9.4) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes==0.43.1) (2.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from pandas->datasets->trl==0.9.4) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from pandas->datasets->trl==0.9.4) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets->trl==0.9.4) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/rocketry/lib/python3.10/site-packages (from sympy->torch->bitsandbytes==0.43.1) (1.3.0)\n",
            "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "Downloading trl-0.9.4-py3-none-any.whl (226 kB)\n",
            "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl (222.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2025.10.23-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m791.5/791.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n",
            "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
            "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
            "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typeguard, triton, shtab, regex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mdurl, docstring-parser, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, tokenizers, rich, nvidia-cusolver-cu12, tyro, transformers, torch, xformers, bitsandbytes, trl, peft\n",
            "\u001b[2K  Attempting uninstall: triton\n",
            "\u001b[2K    Found existing installation: triton 3.1.0\n",
            "\u001b[2K    Uninstalling triton-3.1.0:\n",
            "\u001b[2K      Successfully uninstalled triton-3.1.0\n",
            "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m22/28\u001b[0m [transformers]er-cu12]2]2]\n",
            "\u001b[2K    Found existing installation: torch 2.5.1╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m22/28\u001b[0m [transformers]\n",
            "\u001b[2K    Uninstalling torch-2.5.1:━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m22/28\u001b[0m [transformers]\n",
            "\u001b[2K      Successfully uninstalled torch-2.5.10m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m23/28\u001b[0m [torch]rs]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/28\u001b[0m [peft]2m27/28\u001b[0m [peft]ndbytes]\n",
            "\u001b[1A\u001b[2KSuccessfully installed bitsandbytes-0.43.1 docstring-parser-0.17.0 markdown-it-py-4.0.0 mdurl-0.1.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 peft-0.11.1 regex-2025.10.23 rich-14.2.0 shtab-1.7.2 tokenizers-0.19.1 torch-2.3.0 transformers-4.41.2 triton-2.3.0 trl-0.9.4 typeguard-4.4.4 tyro-0.9.35 xformers-0.0.26.post1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes==0.43.1 peft==0.11.1 trl==0.9.4 transformers==4.41.2 xformers==0.0.26.post1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi8nA_dfkm3W"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Set maximum sequence length - allows for longer paper sections + JSON output\n",
        "# While we set 30000, actual training time depends on the length of data samples (~10k in your case).\n",
        "# This primarily reserves VRAM.\n",
        "max_seq_length = 60000\n",
        "dtype = None # None for auto detection (will likely be float16 on T4/V100, bfloat16 on Ampere+)\n",
        "load_in_4bit = True # Use 4bit quantization for memory efficiency\n",
        "\n",
        "# REASON: Must use the 'Instruct' model as your dataset uses the chat format.\n",
        "# We use the 4bit quantized version optimized by Unsloth for lower memory usage.\n",
        "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "\n",
        "# Load the model and tokenizer using Unsloth's optimized function\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # Add your Hugging Face token if using gated models like original Llama 2/3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOjMypl3koeo"
      },
      "outputs": [],
      "source": [
        "# Apply LoRA adapters to the model for efficient fine-tuning (QLoRA)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    # REASON: r = 16 is a balanced choice for adapter rank (capacity).\n",
        "    # Balances learning capability with memory usage and overfitting risk.\n",
        "    r = 16,\n",
        "\n",
        "    # Specify which modules (layers) to apply LoRA to. These are common choices for Llama models.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16, # Scaling factor for LoRA updates\n",
        "    lora_dropout = 0, # Set dropout to 0 (optimized in Unsloth)\n",
        "    bias = \"none\",    # Use no bias term (optimized in Unsloth)\n",
        "\n",
        "    # Use Unsloth's custom gradient checkpointing for better memory efficiency with long sequences\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407, # For reproducibility\n",
        "    use_rslora = False,  # Rank Stabilized LoRA (optional)\n",
        "    loftq_config = None, # LoftQ initialization (optional)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1WWdjG3kqJH"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# --- IMPORTANT ---\n",
        "# 1. Make sure you've uploaded your 'train.jsonl' file to the Colab environment.\n",
        "#    Use the \"Files\" tab on the left sidebar.\n",
        "# -------------------\n",
        "\n",
        "# REASON: Load your custom dataset directly.\n",
        "# The `load_dataset` function handles the .jsonl format automatically.\n",
        "dataset_path = \"train.jsonl\" # Make sure this matches the uploaded filename\n",
        "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "\n",
        "# Optional: Print the first sample to verify it loaded correctly\n",
        "print(\"--- Example Data Sample ---\")\n",
        "print(dataset[0])\n",
        "print(\"---------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx6c01i9kr5L"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Configure the trainer for Supervised Fine-Tuning\n",
        "trainer = SFTTrainer(\n",
        "    model = model, # The QLoRA-adapted model\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset, # Your loaded dataset\n",
        "\n",
        "    # REASON: No 'dataset_text_field' needed. SFTTrainer automatically detects\n",
        "    # and uses the 'messages' format when it finds it in the dataset.\n",
        "\n",
        "    max_seq_length = max_seq_length, # The max token limit (30000)\n",
        "    dataset_num_proc = 2, # Number of CPU cores for pre-processing (adjust based on Colab instance)\n",
        "    packing = False, # Set to True if most sequences are much shorter than max_seq_length,\n",
        "                     # can speed up training but might behave differently with very long sequences.\n",
        "                     # False is safer for potentially long sequences.\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        # REASON: Batch size 2 and grad accumulation 4 for effective batch size 8.\n",
        "        # This helps fit into memory while stabilizing training.\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "\n",
        "        warmup_steps = 5, # Number of steps for learning rate warmup\n",
        "\n",
        "        # REASON: Train for 3 full passes over your 350 samples.\n",
        "        # Good starting point for a small dataset to avoid under/overfitting.\n",
        "        num_train_epochs = 3,\n",
        "\n",
        "        learning_rate = 2e-4, # Standard learning rate for LoRA\n",
        "        fp16 = not is_bfloat16_supported(), # Use mixed-precision FP16 if BF16 is not supported\n",
        "        bf16 = is_bfloat16_supported(),    # Use BF16 if supported (better for training stability)\n",
        "\n",
        "        # REASON: Log training loss every 10 steps for clarity.\n",
        "        logging_steps = 10,\n",
        "\n",
        "        optim = \"adamw_8bit\", # Memory-efficient AdamW optimizer\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\", # Simple linear learning rate decay\n",
        "        seed = 3407, # For reproducibility\n",
        "        output_dir = \"outputs\", # Directory to save checkpoints\n",
        "        report_to = \"none\", # Disable reporting to Weights & Biases (can be set to \"wandb\")\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2Bn8smykxtv"
      },
      "outputs": [],
      "source": [
        "# Check initial GPU memory usage before training starts\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqluGVsyk00x"
      },
      "outputs": [],
      "source": [
        "# This command begins the training process based on the trainer configuration.\n",
        "print(\"Starting fine-tuning...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"Fine-tuning finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xOxL26Gk28A"
      },
      "outputs": [],
      "source": [
        "# Display memory usage and time taken after training completes\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "# Check if train_runtime exists in metrics, handle potential errors if training stopped early\n",
        "train_runtime = trainer_stats.metrics.get('train_runtime', 0)\n",
        "train_runtime_minutes = round(train_runtime / 60, 2) if train_runtime > 0 else 0\n",
        "\n",
        "print(f\"{train_runtime:.4f} seconds used for training.\")\n",
        "print(f\"{train_runtime_minutes} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr_YkTejk6vX"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "# Prepare the model for faster inference after training\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# --- Define your test input ---\n",
        "system_prompt = \"You are an expert AI assistant. Your task is to read the provided research paper text and generate a JSON object that represents the core logic of the paper as a flowchart, including nodes and edges.\"\n",
        "user_input = \"The Transformer architecture relies on self-attention mechanisms...\" # Replace with a short snippet from a paper NOT in your training set\n",
        "\n",
        "# --- Format using the chat template ---\n",
        "# REASON: Crucial for Instruct models. Must format the input exactly as the model expects.\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_input},\n",
        "    # Leave assistant content empty for generation\n",
        "]\n",
        "\n",
        "# Apply the template to create the input IDs\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Important! Tells the model to generate the assistant response\n",
        "    return_tensors = \"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# --- Generate the response ---\n",
        "# Generate text using the model\n",
        "outputs = model.generate(input_ids=inputs, max_new_tokens = 1024, use_cache = True) # Increased max_new_tokens for potentially long JSON\n",
        "\n",
        "# Decode the generated tokens back into text\n",
        "response_full = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
        "\n",
        "# --- Print only the generated part ---\n",
        "# Find the start of the assistant's response and print it cleanly\n",
        "assistant_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
        "response_start_index = response_full.find(assistant_start_tag)\n",
        "\n",
        "print(\"--- Model Input Prompt ---\")\n",
        "print(tokenizer.decode(inputs[0], skip_special_tokens=False)) # Show the formatted input\n",
        "print(\"\\n--- Generated Response ---\")\n",
        "\n",
        "if response_start_index != -1:\n",
        "    assistant_response = response_full[response_start_index + len(assistant_start_tag):]\n",
        "    # Remove the end-of-turn token if present\n",
        "    assistant_response = assistant_response.replace(\"<|eot_id|>\", \"\").strip()\n",
        "    print(assistant_response)\n",
        "else:\n",
        "    print(\"Could not find assistant start tag in the response.\")\n",
        "    print(\"Full response:\", response_full) # Print full for debugging if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BumY4qWxk81i"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "FastLanguageModel.for_inference(model) # Ensure model is in inference mode\n",
        "\n",
        "# --- Define another test input ---\n",
        "messages_stream = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert AI assistant...\"}, # Use your system prompt\n",
        "    {\"role\": \"user\", \"content\": \"Provide a brief summary of Proximal Policy Optimization (PPO).\"}, # Another test query\n",
        "]\n",
        "\n",
        "# --- Apply template ---\n",
        "inputs_stream = tokenizer.apply_chat_template(\n",
        "    messages_stream,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# --- Setup streamer and generate ---\n",
        "# skip_prompt=True ensures only the newly generated tokens are printed\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "print(\"--- Streaming Response ---\")\n",
        "_ = model.generate(input_ids=inputs_stream, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1_81itYk-25"
      },
      "outputs": [],
      "source": [
        "# Save the trained LoRA adapter weights locally to the 'lora_model' directory\n",
        "print(\"Saving LoRA adapters...\")\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "print(\"Adapters saved locally to 'lora_model'.\")\n",
        "\n",
        "# --- Optional: Push to Hugging Face Hub ---\n",
        "# Make sure you have logged in using !huggingface-cli login\n",
        "# Replace \"your_username/your_model_name\" with your desired HF repo name\n",
        "# model.push_to_hub(\"your_username/your_model_name\", token = \"YOUR_HF_TOKEN\")\n",
        "# tokenizer.push_to_hub(\"your_username/your_model_name\", token = \"YOUR_HF_TOKEN\")\n",
        "# print(\"Adapters pushed to Hugging Face Hub.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rocketry",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
