{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ualNlhI7ZaJk"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries for Unsloth, Hugging Face, etc.\n",
        "# Uses specific versions optimized for Colab in the 'else' block.\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Set maximum sequence length - allows for longer paper sections + JSON output\n",
        "# While we set 30000, actual training time depends on the length of data samples (~10k in your case).\n",
        "# This primarily reserves VRAM.\n",
        "max_seq_length = 30000\n",
        "dtype = None # None for auto detection (will likely be float16 on T4/V100, bfloat16 on Ampere+)\n",
        "load_in_4bit = True # Use 4bit quantization for memory efficiency\n",
        "\n",
        "# REASON: Must use the 'Instruct' model as your dataset uses the chat format.\n",
        "# We use the 4bit quantized version optimized by Unsloth for lower memory usage.\n",
        "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "\n",
        "# Load the model and tokenizer using Unsloth's optimized function\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # Add your Hugging Face token if using gated models like original Llama 2/3\n",
        ")"
      ],
      "metadata": {
        "id": "vi8nA_dfkm3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA adapters to the model for efficient fine-tuning (QLoRA)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    # REASON: r = 16 is a balanced choice for adapter rank (capacity).\n",
        "    # Balances learning capability with memory usage and overfitting risk.\n",
        "    r = 16,\n",
        "\n",
        "    # Specify which modules (layers) to apply LoRA to. These are common choices for Llama models.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16, # Scaling factor for LoRA updates\n",
        "    lora_dropout = 0, # Set dropout to 0 (optimized in Unsloth)\n",
        "    bias = \"none\",    # Use no bias term (optimized in Unsloth)\n",
        "\n",
        "    # Use Unsloth's custom gradient checkpointing for better memory efficiency with long sequences\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407, # For reproducibility\n",
        "    use_rslora = False,  # Rank Stabilized LoRA (optional)\n",
        "    loftq_config = None, # LoftQ initialization (optional)\n",
        ")"
      ],
      "metadata": {
        "id": "MOjMypl3koeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# --- IMPORTANT ---\n",
        "# 1. Make sure you've uploaded your 'train.jsonl' file to the Colab environment.\n",
        "#    Use the \"Files\" tab on the left sidebar.\n",
        "# -------------------\n",
        "\n",
        "# REASON: Load your custom dataset directly.\n",
        "# The `load_dataset` function handles the .jsonl format automatically.\n",
        "dataset_path = \"train.jsonl\" # Make sure this matches the uploaded filename\n",
        "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "\n",
        "# Optional: Print the first sample to verify it loaded correctly\n",
        "print(\"--- Example Data Sample ---\")\n",
        "print(dataset[0])\n",
        "print(\"---------------------------\")"
      ],
      "metadata": {
        "id": "h1WWdjG3kqJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Configure the trainer for Supervised Fine-Tuning\n",
        "trainer = SFTTrainer(\n",
        "    model = model, # The QLoRA-adapted model\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset, # Your loaded dataset\n",
        "\n",
        "    # REASON: No 'dataset_text_field' needed. SFTTrainer automatically detects\n",
        "    # and uses the 'messages' format when it finds it in the dataset.\n",
        "\n",
        "    max_seq_length = max_seq_length, # The max token limit (30000)\n",
        "    dataset_num_proc = 2, # Number of CPU cores for pre-processing (adjust based on Colab instance)\n",
        "    packing = False, # Set to True if most sequences are much shorter than max_seq_length,\n",
        "                     # can speed up training but might behave differently with very long sequences.\n",
        "                     # False is safer for potentially long sequences.\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        # REASON: Batch size 2 and grad accumulation 4 for effective batch size 8.\n",
        "        # This helps fit into memory while stabilizing training.\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "\n",
        "        warmup_steps = 5, # Number of steps for learning rate warmup\n",
        "\n",
        "        # REASON: Train for 3 full passes over your 350 samples.\n",
        "        # Good starting point for a small dataset to avoid under/overfitting.\n",
        "        num_train_epochs = 3,\n",
        "\n",
        "        learning_rate = 2e-4, # Standard learning rate for LoRA\n",
        "        fp16 = not is_bfloat16_supported(), # Use mixed-precision FP16 if BF16 is not supported\n",
        "        bf16 = is_bfloat16_supported(),    # Use BF16 if supported (better for training stability)\n",
        "\n",
        "        # REASON: Log training loss every 10 steps for clarity.\n",
        "        logging_steps = 10,\n",
        "\n",
        "        optim = \"adamw_8bit\", # Memory-efficient AdamW optimizer\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\", # Simple linear learning rate decay\n",
        "        seed = 3407, # For reproducibility\n",
        "        output_dir = \"outputs\", # Directory to save checkpoints\n",
        "        report_to = \"none\", # Disable reporting to Weights & Biases (can be set to \"wandb\")\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "Sx6c01i9kr5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check initial GPU memory usage before training starts\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "G2Bn8smykxtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This command begins the training process based on the trainer configuration.\n",
        "print(\"Starting fine-tuning...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"Fine-tuning finished!\")"
      ],
      "metadata": {
        "id": "OqluGVsyk00x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display memory usage and time taken after training completes\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "# Check if train_runtime exists in metrics, handle potential errors if training stopped early\n",
        "train_runtime = trainer_stats.metrics.get('train_runtime', 0)\n",
        "train_runtime_minutes = round(train_runtime / 60, 2) if train_runtime > 0 else 0\n",
        "\n",
        "print(f\"{train_runtime:.4f} seconds used for training.\")\n",
        "print(f\"{train_runtime_minutes} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "3xOxL26Gk28A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "# Prepare the model for faster inference after training\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# --- Define your test input ---\n",
        "system_prompt = \"You are an expert AI assistant. Your task is to read the provided research paper text and generate a JSON object that represents the core logic of the paper as a flowchart, including nodes and edges.\"\n",
        "user_input = \"The Transformer architecture relies on self-attention mechanisms...\" # Replace with a short snippet from a paper NOT in your training set\n",
        "\n",
        "# --- Format using the chat template ---\n",
        "# REASON: Crucial for Instruct models. Must format the input exactly as the model expects.\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_input},\n",
        "    # Leave assistant content empty for generation\n",
        "]\n",
        "\n",
        "# Apply the template to create the input IDs\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Important! Tells the model to generate the assistant response\n",
        "    return_tensors = \"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# --- Generate the response ---\n",
        "# Generate text using the model\n",
        "outputs = model.generate(input_ids=inputs, max_new_tokens = 1024, use_cache = True) # Increased max_new_tokens for potentially long JSON\n",
        "\n",
        "# Decode the generated tokens back into text\n",
        "response_full = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
        "\n",
        "# --- Print only the generated part ---\n",
        "# Find the start of the assistant's response and print it cleanly\n",
        "assistant_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
        "response_start_index = response_full.find(assistant_start_tag)\n",
        "\n",
        "print(\"--- Model Input Prompt ---\")\n",
        "print(tokenizer.decode(inputs[0], skip_special_tokens=False)) # Show the formatted input\n",
        "print(\"\\n--- Generated Response ---\")\n",
        "\n",
        "if response_start_index != -1:\n",
        "    assistant_response = response_full[response_start_index + len(assistant_start_tag):]\n",
        "    # Remove the end-of-turn token if present\n",
        "    assistant_response = assistant_response.replace(\"<|eot_id|>\", \"\").strip()\n",
        "    print(assistant_response)\n",
        "else:\n",
        "    print(\"Could not find assistant start tag in the response.\")\n",
        "    print(\"Full response:\", response_full) # Print full for debugging if needed"
      ],
      "metadata": {
        "id": "vr_YkTejk6vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "FastLanguageModel.for_inference(model) # Ensure model is in inference mode\n",
        "\n",
        "# --- Define another test input ---\n",
        "messages_stream = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert AI assistant...\"}, # Use your system prompt\n",
        "    {\"role\": \"user\", \"content\": \"Provide a brief summary of Proximal Policy Optimization (PPO).\"}, # Another test query\n",
        "]\n",
        "\n",
        "# --- Apply template ---\n",
        "inputs_stream = tokenizer.apply_chat_template(\n",
        "    messages_stream,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# --- Setup streamer and generate ---\n",
        "# skip_prompt=True ensures only the newly generated tokens are printed\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "print(\"--- Streaming Response ---\")\n",
        "_ = model.generate(input_ids=inputs_stream, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"
      ],
      "metadata": {
        "id": "BumY4qWxk81i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained LoRA adapter weights locally to the 'lora_model' directory\n",
        "print(\"Saving LoRA adapters...\")\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "print(\"Adapters saved locally to 'lora_model'.\")\n",
        "\n",
        "# --- Optional: Push to Hugging Face Hub ---\n",
        "# Make sure you have logged in using !huggingface-cli login\n",
        "# Replace \"your_username/your_model_name\" with your desired HF repo name\n",
        "# model.push_to_hub(\"your_username/your_model_name\", token = \"YOUR_HF_TOKEN\")\n",
        "# tokenizer.push_to_hub(\"your_username/your_model_name\", token = \"YOUR_HF_TOKEN\")\n",
        "# print(\"Adapters pushed to Hugging Face Hub.\")"
      ],
      "metadata": {
        "id": "W1_81itYk-25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}